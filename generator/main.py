# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
import os
import request
import json
import config
import myselenium

version = 'v2'
outputdir = version  # è¾“å‡ºæ–‡ä»¶ç»“æ„å˜åŒ–æ—¶ï¼Œæ›´æ–°è¾“å‡ºè·¯å¾„ç‰ˆæœ¬
filenames = []
json_pool = []
baselink = 'https://github.com/'

def fix_label(a):
    b=["example","invalid","Maybe NOT Volantis WARNING","NETWORK ERROR","NETWORK WARNING","NOT HTTPS","NOT Volantis","ğŸ—‘ï¸ Suggest to close","SKIP"]
    #å®šä¹‰ç©ºåˆ—è¡¨
    c=[]
    #range(len(a))å–çš„ä¸ºåˆ—è¡¨açš„ç´¢å¼•ï¼Œæ ¹æ®açš„
    for i in range(len(a)):
        #å–å‡ºç´¢å¼•å¯¹åº”çš„å€¼
        t=a[i]
        #åˆ¤æ–­å€¼æ˜¯å¦å­˜åœ¨åœ¨åºåˆ—bä¸­
        if t in b:
           #å¦‚æœåºåˆ—åœ¨bä¸­ï¼Œåˆ™å†™å…¥åºåˆ—c
           c.append(t)
    return c




def mkdir(path):
    folder = os.path.exists(path)
    if not folder:
        os.makedirs(path)
        print("create dir:", path)
    else:
        print("dir exists:", path)

def getData(repo,parameter,sort,data_pool,json_pool):
    try:
        for number in range(1, 100):
            linklist = []
            print('page:', number)
            url = 'https://github.com/' + repo + '/issues?page=' + str(number) + '&q=is%3Aopen'
            if parameter:
                url = url + parameter
            if sort:
                url = url + '+sort%3A' + sort
            print('parse:', url)
            github = myselenium.get(url)
            soup = BeautifulSoup(github, 'html.parser')
            linklist = soup.find_all('a', {'data-testid': 'issue-pr-title-link'})
            if len(linklist) == 0:
                print('> end')
                break
            for item in linklist:
                issueslink = baselink + item['href']
                issues_page = request.get_data(issueslink)
                issues_soup = BeautifulSoup(issues_page, 'html.parser')
                try:

                    issues_labels = set()
                    issues_labels_a = issues_soup.find_all('span', {'class': 'prc-Text-Text-0ima0'})
                    for i in issues_labels_a:
                      issues_labels.add(i.text.strip())
                    issues_labels=fix_label(list(issues_labels))
                    print(issues_labels)
                    if "NETWORK WARNING" in issues_labels and "SKIP" not in issues_labels:
                        print("skip this.")
                        continue

                    
                    issues_linklist = issues_soup.find_all('pre')
                    source = issues_linklist[0].text
                    if "{" in source:
                        source = json.loads(source)
                        print(source)
                        data_pool.append(source)
                except:
                    continue
    except Exception as e:
        print('> end')
    json_pool.append(data_pool)

def github_issuse(json_pool):
    print('\n')
    print('------- github issues start ----------')
    cfg = config.load()
    filter = cfg['issues']

    if not filter["groups"]:
        # å¦‚æœæ²¡æœ‰é…ç½®groupsï¼Œå…¨éƒ¨è¾“å‡ºè‡³data.json
        data_pool = []
        filenames.append("data")
        parameter='+label%3A' + (filter["label"] if filter["label"] else '')
        getData(filter["repo"],parameter,filter["sort"],data_pool,json_pool)

    else:
        # å¦‚æœé…ç½®å¤šä¸ªäº†groupsï¼ŒæŒ‰ç…§åˆ†ç»„æŠ“å–å¹¶è¾“å‡º
        for group in filter["groups"]:
            print('start of group:', group)
            data_pool = []
            filenames.append(group)
            parameter='+label%3A' + (filter["label"] if filter["label"] else '') + '+label%3A' + group
            getData(filter["repo"],parameter,filter["sort"],data_pool,json_pool)
            print("end of group:", group)

    print('------- github issues end ----------')
    print('\n')


# å‹é“¾è§„åˆ™
github_issuse(json_pool)
mkdir(outputdir)
full_path = []
i = 0
for filename in filenames:
    full_path.append(outputdir + '/' + filename + '.json')
    with open(full_path[i], 'w', encoding='utf-8') as file_obj:
        data = {'version': version, 'content': json_pool[i]}
        json.dump(data, file_obj, ensure_ascii=False, indent=2)
    i += 1
